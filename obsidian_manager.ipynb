{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cargar env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"obsidian_jarvis\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"./obsidian_jarvis_db\",  # Where to save data locally, remove if not necessary\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "obsidian_retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probar vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* [SIM=1.065205] Es una herramienta para controlar LLM's, por ejemplo si una LLM se usa para detectar queries de clientes que introducen texto indicando una cancion pero se necesita otra LLM para escribir la respuesta\n",
      "\n",
      "[[Rag from scratch with langchain]] [{'concepts': 'TEC-PRO', 'id': 101, 'last_modified': '2025-03-03T19:36:13.901802', 'tags': 'Lenguaje de progra', 'title': 'LangChain', 'uuid': '62f34590-740d-4a2b-9c04-523247011b06'}]\n",
      "* [SIM=1.258643] ### Info:\n",
      "Retrieval-augmented generation\n",
      "\n",
      "this is a method to have a data store connected to the LLM in order to review information relevant to the prompt and be able to:\n",
      "- source the data \n",
      "- update the data without retraining the model\n",
      "- say \"i dont know\"\n",
      "\n",
      "\n",
      "### Aprendizaje:\n",
      "[[Rag from scratch with langchain]] [{'concepts': 'TEC-ML-FOU', 'id': 16, 'last_modified': '2025-03-03T19:31:14.906355', 'tags': 'ML', 'title': 'RAG', 'uuid': 'edeb398a-7f94-4844-a4d9-7bf637e42d09'}]\n",
      "* [SIM=1.541880] trabajos sobre\n",
      "\n",
      "## 1. machine learning programming (fine tuning models and parameters)\n",
      "\n",
      "\n",
      "## 2. LLM prompt engineering and [{'concepts': 'TEC-ML-PRO, TEC-ML-FOU', 'id': 231, 'last_modified': '2025-03-03T19:43:03.230157', 'tags': '', 'title': 'actualizar la pagina web', 'uuid': 'a777082a-6e72-4570-aa74-f61d4aabf723'}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "results = vector_store.similarity_search_with_score(\n",
    "    \"what can you tell me about langchain?\", k=3,\n",
    ")\n",
    "for res, score in results:\n",
    "    print(f\"* [SIM={score:3f}] {res.page_content} [{res.metadata}]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM orchestration\n",
    "## Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luisgg/programacion/git/obsidian_agent/Obsidian_agent/venv/lib/python3.11/site-packages/langsmith/client.py:253: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
      "Question: (question goes here) \n",
      "Context: (context goes here) \n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "example_messages = prompt.invoke(\n",
    "    {\"context\": \"(context goes here)\", \"question\": \"(question goes here)\"}\n",
    ").to_messages()\n",
    "\n",
    "assert len(example_messages) == 1\n",
    "print(example_messages[0].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from typing_extensions import List, TypedDict\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using state - mixing it with functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(state: State):\n",
    "    retrieved_docs = vector_store.similarity_search(state[\"question\"])\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langgraph.graph import START, StateGraph\n",
    "\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: [Document(id='e93697c8-c831-4119-a6fe-3a456c5d8f34', metadata={'concepts': 'PER-INF-KM, TEC-SOF, TEC-PRO', 'id': 50, 'last_modified': '2025-03-03T19:33:20.551902', 'tags': 'Estudiar, Software engineering, Examenes de conocimiento, Administración', 'title': 'Posibles proyectos', 'uuid': 'e93697c8-c831-4119-a6fe-3a456c5d8f34'}, page_content='# Software / Profesional\\n\\n[5 proyectos atractivos](https://www.youtube.com/watch?v=vGxR98gI930&ab_channel=JasonGoodison)\\n[Proyectos para hacer segun prime](https://www.youtube.com/watch?v=yeatOU5vVsA&ab_channel=ThePrimeTime)\\n## Aplicaciones\\n[[InteractionAI]]\\n[[Life manager]]\\n## Portafolios\\n[[Leetcode]]\\n## Habilidades\\n[[Git y github]]'), Document(id='e4205bdd-4da1-479b-96e2-3206e271f3ea', metadata={'concepts': 'TEC-PRO', 'id': 180, 'last_modified': '2025-03-03T19:40:28.602801', 'tags': 'Trabajo 💼', 'title': 'Remotask', 'uuid': 'e4205bdd-4da1-479b-96e2-3206e271f3ea'}, page_content='[Remotask](https://www.remotasks.com/dashboard) Es la plataforma que utilizo para trabajar. Actualmente no me encuentro en algun proyecto pero para proyectos como multiturn podemos ver notas como \\n### Herramientas:\\n\\n#### Guia para: Remotask flamingo \\n\\nAdd at least three parameters to a prompt:- Role, Task, Context, Time, Steps,\\xa0 Format, Constraints, and Goal. The goal is not the complexity of the computer science The goal is the complexity of the parameters involved in the prompt. the complexity of the parameters can be increased by adding more of them to the pro\\nmpt. Please do your best to do that without being verbose, you want to confuse the AI so that it deviates and then you can teach it with your responses what is correct.\\n\\n[[Towers of hanoi algorithm]]\\n\\nPromps:\\nI want a program that opens an Excel file and modifies it with Python according to the following rules:\\n- In the first row there will be the column labels, they will have numbers indicating what to do with the whole column.\\n- Columns that begin with a 1 will be normalized by taking the minimum and maximum values and normalizing all the values in that range.\\n- Columns that start with a 2 will be deleted. Dropped.\\n- Columns that start with 3 can be normalized in random ways, for example, -1 or 1, or 1 and 2, these columns may also contain 3 or 4 different values, like 1, 2, 3, 4. Target them by checking if they have 2 values, change them to 1 and 0, and if they have more than 2 normalize them by min and max.\\n- The code must have comments and can use any library like pandas.\\n\\nCuando escriba una respuesta de un prompt:\\n\\n![[Pasted image 20231221213501.png]]'), Document(id='587e03b7-e567-48b4-b48b-6c9a626bc399', metadata={'concepts': 'TEC-ML, TEC-ML-FOU', 'id': 91, 'last_modified': '2025-03-03T19:35:51.417239', 'tags': 'Información, ML', 'title': 'Investigacion sobre metaquest 3', 'uuid': '587e03b7-e567-48b4-b48b-6c9a626bc399'}, page_content='[The Meta Quest 3, like its predecessor the Quest 2, is expected to feature hand tracking](https://www.gfinityesports.com/vr/will-meta-quest-3-have-hand-tracking/)[1](https://www.gfinityesports.com/vr/will-meta-quest-3-have-hand-tracking/).\\xa0[The hand tracking system on the Meta Quest series uses a combination of deep learning techniques and model-based tracking](https://ai.meta.com/blog/hand-tracking-deep-neural-networks/)[2](https://ai.meta.com/blog/hand-tracking-deep-neural-networks/).\\n\\nHere’s a brief overview of how it works:\\n\\n1. [**Hand Detection**: The system uses the Quest’s four monochrome cameras to detect the location of a person’s hands](https://ai.meta.com/blog/hand-tracking-deep-neural-networks/)[2](https://ai.meta.com/blog/hand-tracking-deep-neural-networks/).\\xa0[This is done entirely on-device, without the need for any additional equipment](https://ai.meta.com/blog/hand-tracking-deep-neural-networks/)[2](https://ai.meta.com/blog/hand-tracking-deep-neural-networks/).\\n    \\n2. [**Hand Landmark Prediction**: Deep neural networks are used to predict landmarks on the hands, such as the joints and fingertips](https://ai.meta.com/blog/hand-tracking-deep-neural-networks/)[2](https://ai.meta.com/blog/hand-tracking-deep-neural-networks/).\\xa0[These landmarks are then used to reconstruct a 26 degree-of-freedom pose of the person’s hands and fingers](https://ai.meta.com/blog/hand-tracking-deep-neural-networks/)[2](https://ai.meta.com/blog/hand-tracking-deep-neural-networks/).\\n    \\n3. [**3D Model Creation**: The predicted landmarks and hand pose are used to create a 3D model that includes the configuration and surface geometry of the hand](https://ai.meta.com/blog/hand-tracking-deep-neural-networks/)[2](https://ai.meta.com/blog/hand-tracking-deep-neural-networks/).\\xa0[This model can then be used by developers to enable new interaction mechanics in their apps or to drive a user interface](https://ai.meta.com/blog/hand-tracking-deep-neural-networks/)[2](https://ai.meta.com/blog/hand-tracking-deep-neural-networks/).\\n    \\n4. [**Real-Time Tracking**: The system uses an efficient, quantized neural network framework that enables real-time hand-tracking on a mobile processor](https://ai.meta.com/blog/hand-tracking-deep-neural-networks/)[2](https://ai.meta.com/blog/hand-tracking-deep-neural-networks/).\\xa0[This allows for robust estimates of hand pose across a wide range of environments](https://ai.meta.com/blog/hand-tracking-deep-neural-networks/)[2](https://ai.meta.com/blog/hand-tracking-deep-neural-networks/).\\n    \\n\\n[The use of deep learning in this context allows for accurate, low-jitter estimates of hand pose, making VR experiences feel more natural and intuitive](https://ai.meta.com/blog/hand-tracking-deep-neural-networks/)[2](https://ai.meta.com/blog/hand-tracking-deep-neural-networks/).\\xa0[It also opens up possibilities for new types of interactions in virtual worlds, such as pointing and pinch-to-select gestures](https://ai.meta.com/blog/hand-tracking-deep-neural-networks/)[2](https://ai.meta.com/blog/hand-tracking-deep-neural-networks/).\\n\\nFor more detailed information on the specific deep learning algorithms used, you may want to refer to research papers or technical documents published by Meta. They have done extensive work in this area and have likely published more detailed information on their techniques.'), Document(id='2cca682d-7674-4218-8423-25f08d63100e', metadata={'concepts': 'TEC-SOF, TEC-ELE, TEC-MAT', 'id': 85, 'last_modified': '2025-03-03T19:35:33.238394', 'tags': '', 'title': 'Ingenieria 🦾', 'uuid': '2cca682d-7674-4218-8423-25f08d63100e'}, page_content='[[Software engineering]]\\n[[Electronica - HW]]\\n[[Matematicas]]\\n[[Diseño de hardware o equipos]]\\n\\n## Pruebas a mi mismo\\n[[Examen de regresion lineal (incluye teoria, python y aplicaciones)]]\\n[[Python financial coding challenge]]\\n[[Posibles proyectos]]\\n\\n\\n### Videos sobre ingenieria en general\\n[Finishing SAT Practice Test 6 No Calculator Section (w/ Explanations) in 7 MINUTES](https://www.youtube.com/watch?v=pX8f3TArjpo)\\n[Iron Man Repulsor 2.0 (3000°C HHO torch for Iron Man suit) DIY alkali electrolyzer](https://www.youtube.com/watch?v=LOo5kl0vZtw)\\n[My Brain after 569 Leetcode Problems - YouTube](https://www.youtube.com/watch?v=8wysIxzqgPI)')]\n",
      "\n",
      "\n",
      "Answer: The context mentions several projects and applications, such as InteractionAI and Life Manager, which highlight software development efforts. Tools like Remotask are utilized in various projects, emphasizing a focus on specific tasks and workflow strategies. Additionally, there are algorithms and programming challenges, indicating a strong emphasis on coding and problem-solving skills in the vault's activities.\n"
     ]
    }
   ],
   "source": [
    "result = graph.invoke({\"question\": \"what can you tell me about tecnologies and projectes in the vault?\"})\n",
    "\n",
    "print(f'Context: {result[\"context\"]}\\n\\n')\n",
    "print(f'Answer: {result[\"answer\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upgrade note:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extractor (query analyzer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "# Define a custom prompt to provide instructions and any additional context.\n",
    "# 1) You can add examples into the prompt template to improve extraction quality\n",
    "# 2) Introduce additional parameters to take context into account (e.g., include metadata\n",
    "#    about the document from which the text was extracted.)\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are an expert extraction algorithm. \"\n",
    "            \"Only extract relevant information from the query \"\n",
    "            \"If you do not know the value of an attribute asked \"\n",
    "            \"to extract, return null for the attribute's value.\",\n",
    "        ),\n",
    "        # ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓\n",
    "        MessagesPlaceholder(\"examples\"),  # <-- EXAMPLES!\n",
    "        # ↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑\n",
    "        (\"human\", \"{text}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content=\"You are an expert extraction algorithm. Only extract relevant information from the queryIf you do not know the value of an attribute asked to extract, return null for the attribute's value.\", additional_kwargs={}, response_metadata={}), HumanMessage(content='testing 1 2 3', additional_kwargs={}, response_metadata={}), HumanMessage(content='this is some text', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import (\n",
    "    HumanMessage,\n",
    ")\n",
    "\n",
    "prompt.invoke(\n",
    "    {\"text\": \"this is some text\", \"examples\": [HumanMessage(content=\"testing 1 2 3\")]}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "from typing import List, Optional, Literal\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "TipoOptions = Literal[\"Resolver duda\", \"Elaborar un plan de accion\", \"Realizar una tarea\"]\n",
    "tiempoOptions = Literal[\"Pasado\", \"Presente\", \"Futuro\"]\n",
    "UrgenciaOptions = Literal[\"baja\", \"media\", \"alta\", \"crítica\"]\n",
    "endgoalOptions = Literal[\"Desarrollar informacion sobre un tema\", \"Resolver un problema inmediato\"]\n",
    "\n",
    "class pregunta(BaseModel):\n",
    "    \"\"\"Information about a person.\"\"\"\n",
    "    tipo: Optional[TipoOptions] = Field(..., description=\"Que tipo de accion requiere la consulta?\")\n",
    "    urgencia: Optional[UrgenciaOptions] = Field(\n",
    "        ..., description=\"Requiere una respuesta inmediata y corta o puede esperar una respuesta mas larga y detallada?\"\n",
    "    )\n",
    "    tiempo: Optional[tiempoOptions] = Field(..., description=\"En que tiempo se esta refiriendo, si se consulta informacion pasada o planes para futuro.\")\n",
    "    end_goal: Optional[endgoalOptions] = Field(..., description=\"Que es lo que se quiere lograr con la consulta?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from typing import Dict, List, TypedDict\n",
    "\n",
    "from langchain_core.messages import (\n",
    "    AIMessage,\n",
    "    BaseMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    "    ToolMessage,\n",
    ")\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class Example(TypedDict):\n",
    "    \"\"\"A representation of an example consisting of text input and expected tool calls.\n",
    "\n",
    "    For extraction, the tool calls are represented as instances of pydantic model.\n",
    "    \"\"\"\n",
    "\n",
    "    input: str  # This is the example text\n",
    "    tool_calls: List[BaseModel]  # Instances of pydantic model that should be extracted\n",
    "\n",
    "\n",
    "def tool_example_to_messages(example: Example) -> List[BaseMessage]:\n",
    "    \"\"\"Convert an example into a list of messages that can be fed into an LLM.\n",
    "\n",
    "    This code is an adapter that converts our example to a list of messages\n",
    "    that can be fed into a chat model.\n",
    "\n",
    "    The list of messages per example corresponds to:\n",
    "\n",
    "    1) HumanMessage: contains the content from which content should be extracted.\n",
    "    2) AIMessage: contains the extracted information from the model\n",
    "    3) ToolMessage: contains confirmation to the model that the model requested a tool correctly.\n",
    "\n",
    "    The ToolMessage is required because some of the chat models are hyper-optimized for agents\n",
    "    rather than for an extraction use case.\n",
    "    \"\"\"\n",
    "    messages: List[BaseMessage] = [HumanMessage(content=example[\"input\"])]\n",
    "    tool_calls = []\n",
    "    for tool_call in example[\"tool_calls\"]:\n",
    "        tool_calls.append(\n",
    "            {\n",
    "                \"id\": str(uuid.uuid4()),\n",
    "                \"args\": tool_call.model_dump(),\n",
    "                # The name of the function right now corresponds\n",
    "                # to the name of the pydantic model\n",
    "                # This is implicit in the API right now,\n",
    "                # and will be improved over time.\n",
    "                \"name\": tool_call.__class__.__name__,\n",
    "            },\n",
    "        )\n",
    "    messages.append(AIMessage(content=\"\", tool_calls=tool_calls))\n",
    "    tool_outputs = example.get(\"tool_outputs\") or [\n",
    "        \"You have correctly called this tool.\"\n",
    "    ] * len(tool_calls)\n",
    "    for output, tool_call in zip(tool_outputs, tool_calls):\n",
    "        messages.append(ToolMessage(content=output, tool_call_id=tool_call[\"id\"]))\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    (\n",
    "        \"Quiero aprender sobre matematicas desde donde me habia quedado\",\n",
    "        pregunta(\n",
    "            tipo=\"Resolver duda\",\n",
    "            urgencia=\"baja\", \n",
    "            tiempo=\"Presente\",\n",
    "            end_goal=\"Desarrollar informacion sobre un tema\"\n",
    "        ),\n",
    "    ),\n",
    "    (\n",
    "        \"Se puede hacer agua mineral de forma casera?\",\n",
    "        pregunta(\n",
    "            tipo=\"Resolver duda\",\n",
    "            urgencia=\"alta\",\n",
    "            tiempo=\"Presente\", \n",
    "            end_goal=\"Resolver un problema inmediato\"\n",
    "        ),\n",
    "    ),\n",
    "    (\n",
    "        \"Necesito que recuerdes esto y lo escribas en una nota nueva\",\n",
    "        pregunta(\n",
    "            tipo=\"Realizar una tarea\",\n",
    "            urgencia=\"alta\",\n",
    "            tiempo=\"Presente\",\n",
    "            end_goal=\"Resolver un problema inmediato\"\n",
    "        ),\n",
    "    ),\n",
    "    (\n",
    "        \"Quiero que me ayudes a planificar mi proximo viaje a Japon\",\n",
    "        pregunta(\n",
    "            tipo=\"Elaborar un plan de accion\",\n",
    "            urgencia=\"baja\",\n",
    "            tiempo=\"Futuro\",\n",
    "            end_goal=\"Desarrollar informacion sobre un tema\"\n",
    "        ),\n",
    "    ),\n",
    "    (\n",
    "        \"Am I Cute?\",\n",
    "        pregunta(\n",
    "            tipo=\"Resolver duda\",\n",
    "            urgencia=\"baja\",\n",
    "            tiempo=\"Presente\",\n",
    "            end_goal=\"Resolver un problema inmediato\"\n",
    "        ),\n",
    "    )\n",
    "]\n",
    "\n",
    "messages = []\n",
    "\n",
    "for text, tool_call in examples:\n",
    "    messages.extend(\n",
    "        tool_example_to_messages({\"input\": text, \"tool_calls\": [tool_call]})\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system: content=\"You are an expert extraction algorithm. Only extract relevant information from the query If you do not know the value of an attribute asked to extract, return null for the attribute's value.\" additional_kwargs={} response_metadata={}\n",
      "human: content='Quiero aprender sobre matematicas desde donde me habia quedado' additional_kwargs={} response_metadata={}\n",
      "ai: content='' additional_kwargs={} response_metadata={} tool_calls=[{'name': 'pregunta', 'args': {'tipo': 'Resolver duda', 'urgencia': 'baja', 'tiempo': 'Presente', 'end_goal': 'Desarrollar informacion sobre un tema'}, 'id': '471f3689-76e5-439f-9b47-74c1352bbcd9', 'type': 'tool_call'}]\n",
      "tool: content='You have correctly called this tool.' tool_call_id='471f3689-76e5-439f-9b47-74c1352bbcd9'\n",
      "human: content='Se puede hacer agua mineral de forma casera?' additional_kwargs={} response_metadata={}\n",
      "ai: content='' additional_kwargs={} response_metadata={} tool_calls=[{'name': 'pregunta', 'args': {'tipo': 'Resolver duda', 'urgencia': 'alta', 'tiempo': 'Presente', 'end_goal': 'Resolver un problema inmediato'}, 'id': 'b0da6f53-e972-4ba8-8415-3f2668a307c9', 'type': 'tool_call'}]\n",
      "tool: content='You have correctly called this tool.' tool_call_id='b0da6f53-e972-4ba8-8415-3f2668a307c9'\n",
      "human: content='Necesito que recuerdes esto y lo escribas en una nota nueva' additional_kwargs={} response_metadata={}\n",
      "ai: content='' additional_kwargs={} response_metadata={} tool_calls=[{'name': 'pregunta', 'args': {'tipo': 'Realizar una tarea', 'urgencia': 'alta', 'tiempo': 'Presente', 'end_goal': 'Resolver un problema inmediato'}, 'id': '39af7d74-46ec-4718-ac12-9a7a4a35facd', 'type': 'tool_call'}]\n",
      "tool: content='You have correctly called this tool.' tool_call_id='39af7d74-46ec-4718-ac12-9a7a4a35facd'\n",
      "human: content='Quiero que me ayudes a planificar mi proximo viaje a Japon' additional_kwargs={} response_metadata={}\n",
      "ai: content='' additional_kwargs={} response_metadata={} tool_calls=[{'name': 'pregunta', 'args': {'tipo': 'Elaborar un plan de accion', 'urgencia': 'baja', 'tiempo': 'Futuro', 'end_goal': 'Desarrollar informacion sobre un tema'}, 'id': 'f07f071c-c68e-44a9-9614-799d283e0db5', 'type': 'tool_call'}]\n",
      "tool: content='You have correctly called this tool.' tool_call_id='f07f071c-c68e-44a9-9614-799d283e0db5'\n",
      "human: content='this is some text' additional_kwargs={} response_metadata={}\n"
     ]
    }
   ],
   "source": [
    "example_prompt = prompt.invoke({\"text\": \"this is some text\", \"examples\": messages})\n",
    "\n",
    "for message in example_prompt.messages:\n",
    "    print(f\"{message.type}: {message}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "llm = init_chat_model(\"gpt-4-0125-preview\", model_provider=\"openai\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "runnable = prompt | llm.with_structured_output(\n",
    "    schema=pregunta,\n",
    "    method=\"function_calling\",\n",
    "    include_raw=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tipo='Resolver duda' urgencia='media' tiempo='Presente' end_goal='Desarrollar informacion sobre un tema'\n",
      "tipo='Resolver duda' urgencia='media' tiempo='Presente' end_goal='Desarrollar informacion sobre un tema'\n",
      "tipo='Resolver duda' urgencia='media' tiempo='Presente' end_goal='Desarrollar informacion sobre un tema'\n",
      "tipo='Resolver duda' urgencia='media' tiempo='Presente' end_goal='Desarrollar informacion sobre un tema'\n",
      "tipo='Resolver duda' urgencia='media' tiempo='Presente' end_goal='Desarrollar informacion sobre un tema'\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    text = \"quiero investigar sobre langchain y necesito tu ayuda\"\n",
    "    print(runnable.invoke({\"text\": text, \"examples\": messages}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pregunta(tipo='Resolver duda', urgencia='baja', tiempo='Presente', end_goal='Resolver un problema inmediato')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runnable.invoke(\n",
    "    {\n",
    "        \"text\": \"Am I cute?\",\n",
    "        \"examples\": messages,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analizador de notas (extraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(Document(id='62f34590-740d-4a2b-9c04-523247011b06', metadata={'concepts': 'TEC-PRO', 'id': 101, 'last_modified': '2025-03-03T19:36:13.901802', 'tags': 'Lenguaje de progra', 'title': 'LangChain', 'uuid': '62f34590-740d-4a2b-9c04-523247011b06'}, page_content=\"Es una herramienta para controlar LLM's, por ejemplo si una LLM se usa para detectar queries de clientes que introducen texto indicando una cancion pero se necesita otra LLM para escribir la respuesta\\n\\n[[Rag from scratch with langchain]]\"), 1.0652047395706177), (Document(id='edeb398a-7f94-4844-a4d9-7bf637e42d09', metadata={'concepts': 'TEC-ML-FOU', 'id': 16, 'last_modified': '2025-03-03T19:31:14.906355', 'tags': 'ML', 'title': 'RAG', 'uuid': 'edeb398a-7f94-4844-a4d9-7bf637e42d09'}, page_content='### Info:\\nRetrieval-augmented generation\\n\\nthis is a method to have a data store connected to the LLM in order to review information relevant to the prompt and be able to:\\n- source the data \\n- update the data without retraining the model\\n- say \"i dont know\"\\n\\n\\n### Aprendizaje:\\n[[Rag from scratch with langchain]]'), 1.2586427927017212), (Document(id='a777082a-6e72-4570-aa74-f61d4aabf723', metadata={'concepts': 'TEC-ML-PRO, TEC-ML-FOU', 'id': 231, 'last_modified': '2025-03-03T19:43:03.230157', 'tags': '', 'title': 'actualizar la pagina web', 'uuid': 'a777082a-6e72-4570-aa74-f61d4aabf723'}, page_content='trabajos sobre\\n\\n## 1. machine learning programming (fine tuning models and parameters)\\n\\n\\n## 2. LLM prompt engineering and'), 1.5418799189691785), (Document(id='ef6396ff-2d62-4351-a2f3-6a89691ef9f2', metadata={'concepts': 'PER-LEA', 'id': 15, 'last_modified': '2025-03-03T19:31:12.898745', 'tags': 'Language learning', 'title': 'Language learning with ChatGPT', 'uuid': 'ef6396ff-2d62-4351-a2f3-6a89691ef9f2'}, page_content='[00:00](https://www.youtube.com/watch?v=BxzKkCRBBOI&t=0s) Introduction and motivation to learn a second language \\n[01:03](https://www.youtube.com/watch?v=BxzKkCRBBOI&t=63s) Creating a structured learning plan with ChatGPT\\n[01:38](https://www.youtube.com/watch?v=BxzKkCRBBOI&t=98s) Finding resources for language immersion\\n[02:23](https://www.youtube.com/watch?v=BxzKkCRBBOI&t=143s) Learning the most common words in the language \\n[03:41](https://www.youtube.com/watch?v=BxzKkCRBBOI&t=221s) Practicing conversation with ChatGPT\\n[04:31](https://www.youtube.com/watch?v=BxzKkCRBBOI&t=271s) Using ChatGPT for grammar learning\\n[06:06](https://www.youtube.com/watch?v=BxzKkCRBBOI&t=366s) Engaging in creative writing and games with ChatGPT\\n[08:01](https://www.youtube.com/watch?v=BxzKkCRBBOI&t=481s) Testing language learning knowledge with ChatGPT\\n[09:01](https://www.youtube.com/watch?v=BxzKkCRBBOI&t=541s) Conclusion and tips for language learning'), 1.582057237625122), (Document(id='9386683e-c73a-4b31-8727-2685bcf2f726', metadata={'concepts': 'TEC-SOF, PER-INF-KM', 'id': 17, 'last_modified': '2025-03-03T19:31:19.405366', 'tags': 'Software engineering, Administración', 'title': 'Leetcode', 'uuid': '9386683e-c73a-4b31-8727-2685bcf2f726'}, page_content=\"[Explore - LeetCode](https://leetcode.com/explore/featured/card/the-leetcode-beginners-guide/)\\n\\n\\nHere's my [[Accounts]] on [Leetcode](https://leetcode.com/).\\n\\n###### I'm currently following this [Roadmap](https://neetcode.io/roadmap).\"), 1.610921025276184)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "results = vector_store.similarity_search_with_score(\n",
    "    \"what can you tell me about langchain?\", k=5,\n",
    ")\n",
    "\n",
    "print(results)\n",
    "\n",
    "#for res, score in results:\n",
    "#    print(f\"* [SIM={score:3f}] {res.page_content} [{res.metadata}]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    VaultFindings: Document\n",
    "    Query_results: pregunta\n",
    "    webfindings: Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import ToolMessage\n",
    "from langchain_core.tools import InjectedToolCallId, tool\n",
    "\n",
    "from langgraph.types import Command, interrupt\n",
    "\n",
    "\n",
    "@tool\n",
    "# Note that because we are generating a ToolMessage for a state update, we\n",
    "# generally require the ID of the corresponding tool call. We can use\n",
    "# LangChain's InjectedToolCallId to signal that this argument should not\n",
    "# be revealed to the model in the tool's schema.\n",
    "def human_assistance(\n",
    "    VaultFindings: str, Web_results: str, tool_call_id: Annotated[str, InjectedToolCallId]\n",
    ") -> str:\n",
    "    \"\"\"Request assistance from a human.\"\"\"\n",
    "    human_response = interrupt(\n",
    "        {\n",
    "            \"question\": \"Is this correct?\",\n",
    "            \"name\": VaultFindings,\n",
    "            \"birthday\": Web_results,\n",
    "        },\n",
    "    )\n",
    "    # If the information is correct, update the state as-is.\n",
    "    if human_response.get(\"correct\", \"\").lower().startswith(\"y\"):\n",
    "        verified_documents = VaultFindings\n",
    "        verified_search_data = Web_results\n",
    "        response = \"Correct\"\n",
    "    # Otherwise, receive information from the human reviewer.\n",
    "    else:\n",
    "        verified_documents = human_response.get(\"name\", VaultFindings)\n",
    "        verified_search_data = human_response.get(\"birthday\", Web_results)\n",
    "        response = f\"Made a correction: {human_response}\"\n",
    "\n",
    "    # This time we explicitly update the state with a ToolMessage inside\n",
    "    # the tool.\n",
    "    state_update = {\n",
    "        \"name\": verified_documents,\n",
    "        \"birthday\": verified_search_data,\n",
    "        \"messages\": [ToolMessage(response, tool_call_id=tool_call_id)],\n",
    "    }\n",
    "    # We return a Command object in the tool to update our state.\n",
    "    return Command(update=state_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "obsidian_retriever = vector_store.as_retriever(\n",
    "    search_kwargs={\"k\": 5}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@tool\n",
    "def extract_vault_data(\n",
    "    tool_call_id: Annotated[str, InjectedToolCallId]\n",
    ") -> str:\n",
    "    \"\"\"Extract data from vault findings and prepare for updates.\"\"\"\n",
    "    # This tool doesn't need parameters since it reads from state\n",
    "    \n",
    "    # Return a Command to update the state\n",
    "    return Command(update={\n",
    "        \"messages\": [ToolMessage(\"Vault data processed successfully\", tool_call_id=tool_call_id)]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a node function that processes vault documents\n",
    "def vault_processor(state: State):\n",
    "    \"\"\"Node to process retrieved vault documents and extract relevant information.\"\"\"\n",
    "    # Extract documents from state\n",
    "    vault_findings = state.get(\"VaultFindings\", [])\n",
    "    query_intent = state.get(\"Query_results\", {})\n",
    "    \n",
    "    # Process documents - here we're keeping it simple as you requested\n",
    "    processed_docs = []\n",
    "    \n",
    "    for doc in vault_findings:\n",
    "        # Extract key information\n",
    "        uuid = doc.metadata.get(\"uuid\", \"\")\n",
    "        title = doc.metadata.get(\"title\", \"Untitled\")\n",
    "        content = doc.page_content\n",
    "        \n",
    "        # Store in a simple dict\n",
    "        processed_docs.append({\n",
    "            \"uuid\": uuid,\n",
    "            \"title\": title,\n",
    "            \"content\": content,\n",
    "            \"metadata\": doc.metadata\n",
    "        })\n",
    "    \n",
    "    # Return updated state\n",
    "    return {\n",
    "        \"processed_vault_docs\": processed_docs,\n",
    "        \"primary_doc\": processed_docs[0] if processed_docs else None\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "\n",
    "webtool = TavilySearchResults(max_results=7)\n",
    "tools = [webtool, human_assistance, extract_vault_data]  # Add the new tool\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "\n",
    "def chatbot(state: State):\n",
    "    message = llm_with_tools.invoke(state[\"messages\"])\n",
    "    assert len(message.tool_calls) <= 1\n",
    "    return {\"messages\": [message]}\n",
    "\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "graph_builder.add_node(\"vault_processor\", vault_processor)  # Add vault processor node\n",
    "\n",
    "tool_node = ToolNode(tools=tools)\n",
    "graph_builder.add_node(\"tools\", tool_node)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a condition to route to vault processor when we have vault findings\n",
    "def should_process_vault(state):\n",
    "    \"\"\"Route to vault processor if we have VaultFindings.\"\"\"\n",
    "    if state.get(\"VaultFindings\"):\n",
    "        return \"vault_processor\"\n",
    "    return \"chatbot\"\n",
    "\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"chatbot\",\n",
    "    tools_condition,\n",
    ")\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"tools\",\n",
    "    should_process_vault,\n",
    ")\n",
    "graph_builder.add_edge(\"vault_processor\", \"chatbot\")\n",
    "graph_builder.add_edge(START, \"chatbot\")\n",
    "\n",
    "memory = MemorySaver()\n",
    "graph = graph_builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "\n",
    "webtool = TavilySearchResults(max_results=7)\n",
    "tools = [webtool, human_assistance]\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "\n",
    "def chatbot(state: State):\n",
    "    message = llm_with_tools.invoke(state[\"messages\"])\n",
    "    assert len(message.tool_calls) <= 1\n",
    "    return {\"messages\": [message]}\n",
    "\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "\n",
    "tool_node = ToolNode(tools=tools)\n",
    "graph_builder.add_node(\"tools\", tool_node)\n",
    "\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"chatbot\",\n",
    "    tools_condition,\n",
    ")\n",
    "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
    "graph_builder.add_edge(START, \"chatbot\")\n",
    "\n",
    "memory = MemorySaver()\n",
    "graph = graph_builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Can you look up when LangGraph was released? When you have the answer, use the human_assistance tool for review.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  tavily_search_results_json (call_3WENbvzlrtsiRD6kfbM1gbqr)\n",
      " Call ID: call_3WENbvzlrtsiRD6kfbM1gbqr\n",
      "  Args:\n",
      "    query: LangGraph release date\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: tavily_search_results_json\n",
      "\n",
      "[{\"title\": \"Releases · langchain-ai/langgraph - GitHub\", \"url\": \"https://github.com/langchain-ai/langgraph/releases\", \"content\": \"Releases · langchain-ai/langgraph · GitHub Search code, repositories, users, issues, pull requests... Releases: langchain-ai/langgraph Releases · langchain-ai/langgraph langgraph: release 0.2.70 (#3341) langgraph: add agent name to AI messages in create_react_agent (#3340) fix(langgraph): Dedupe input (right-side) messages in add_messages (#3338) Merge branch 'jacob/dedupe' of github.com:langchain-ai/langgraph into jacob/dedupe langgraph: release 0.2.69 (#3256) docs: update README to include built w/ langgraph (#3254) langgraph: add get_stream_writer() (#3251) langgraph: release 0.2.68 (#3224) langgraph: actually fix flaky test (#3219) langgraph: use 'prompt' param for model input preprocessing in create_react_agent (#3173) langgraph: fix flaky test (#3218) langgraph: add names for tasks (#3202) langgraph: update docstrings/api ref for functional api (#3176) langgraph: add support for BaseModel updates to Command (#2747) langgraph: allow async state modifier in create_react_agent (#3161)\", \"score\": 0.72418857}, {\"title\": \"langgraph · PyPI\", \"url\": \"https://pypi.org/project/langgraph/\", \"content\": \"LangGraph is a library for building stateful, multi-actor applications with LLMs, used to create agent and multi-agent workflows. The simplest way to create a tool-calling agent in LangGraph is to use create_react_agent: # Define the tools for the agent to use # Define the tools for the agent to use # This means that after `tools` is called, `agent` node is called next. workflow.add_edge(\\\"tools\\\", 'agent') Normal edge: after the tools are invoked, the graph should always return to the agent to decide what to do next LangGraph adds the input message to the internal state, then passes the state to the entrypoint node, \\\"agent\\\". langgraph-0.2.70-py3-none-any.whl (149.7 kB view details)Uploaded Feb 6, 2025 Python 3\", \"score\": 0.68571854}, {\"title\": \"LangGraph Studio release timeline : r/LangChain - Reddit\", \"url\": \"https://www.reddit.com/r/LangChain/comments/1fo4mp4/langgraph_studio_release_timeline/\", \"content\": \"LangGraph Studio is available in beta for only Apple Silicon chip, so when it will be released for other operating system platform like windows and linux?\", \"score\": 0.4330947}, {\"title\": \"January 2024 - LangChain - Changelog\", \"url\": \"https://changelog.langchain.com/?date=2024-01-01\", \"content\": \"LangChain - Changelog LangChain LangSmith LangGraph Blog Case Studies LangChain Academy Community Experts Changelog LangChain LangSmith LangGraph LangChain LangSmith LangGraph LangChain Changelog This splits up the previous langchain package into three different... LangChain 🚀 OpenGPTs ----------- After OpenAI’s DevDay, we launched a project inspired by GPTs and the Assistants API. LangChain 📊 LangChain Benchmarks for Python ---------------------------------- LangChain benchmarks is a Python package with associated datasets to facilitate experimentation and benchmarking of different cognitive architectures. LangSmith SaaS ✍️ Data Annotation Queues in LangSmith -------------------------------------- We've launched our newest feature, data annotation queues, in LangSmith (our SaaS platform for managing your LangChain applications). Subscribe By clicking subscribe, you accept our privacy policy and terms and conditions.\", \"score\": 0.33141232}, {\"title\": \"Announcing LangGraph v0.1 & LangGraph Cloud: Running agents ...\", \"url\": \"https://blog.langchain.dev/langgraph-cloud/\", \"content\": \"Announcing LangGraph v0.1 & LangGraph Cloud: Running agents at scale, reliably Announcing LangGraph v0.1 & LangGraph Cloud: Running agents at scale, reliably Our new infrastructure for running agents at scale, LangGraph Cloud, is available in beta. Separate from the langchain package, LangGraph’s core design philosophy is to help developers add better precision and control into agentic workflows, suitable for the complexity of real-world systems. The next chapter in building complex production-ready features with LLMs is agentic, and with LangGraph and LangSmith, LangChain delivers an out-of-the-box solution to iterate quickly, debug immediately, and scale effortlessly.\\\" – Garrett Spong (Principal SWE @ Elastic) LangGraph Cloud: Scalable agent deployment with integrated monitoring LangGraph Cloud also brings a more integrated experience for collaborating on, deploying, and monitoring your agentic app.\", \"score\": 0.3223558}, {\"title\": \"LangGraph Studio: The first agent IDE - LangChain Blog\", \"url\": \"https://blog.langchain.dev/langgraph-studio-the-first-agent-ide/\", \"content\": \"LangGraph Studio: The first agent IDE LangGraph Studio provides a specialized agent IDE for visualizing, interacting with, and debugging complex agentic applications. Today, we're announcing LangGraph Studio - the first IDE designed specifically for agent development - in open beta. LangGraph Studio offers a new way to develop LLM applications, providing a specialized agent IDE for visualizing, interacting with, and debugging complex agentic applications. In this blog, we'll give a brief overview of LangGraph and then explore how LangGraph Studio streamlines the development of agentic applications. LangGraph Studio: Visualize and interact with agent graphs for quick iteration LangGraph Studio facilitates this by making it easy to visualize and interact with agent graphs, even if development still primarily happens in code.\", \"score\": 0.26226795}, {\"title\": \"LangGraph - LangChain\", \"url\": \"https://www.langchain.com/langgraph\", \"content\": \"Build and scale agentic applications with LangGraph Platform. Design agent-driven user experiences with LangGraph Platform's APIs. Quickly deploy and scale your application with infrastructure built for agents. LangGraph sets the foundation for how we can build and scale AI workloads — from conversational agents, complex task automation, to custom LLM-backed experiences that 'just work'. The next chapter in building complex production-ready features with LLMs is agentic, and with LangGraph and LangSmith, LangChain delivers an out-of-the-box solution to iterate quickly, debug immediately, and scale effortlessly.” LangGraph sets the foundation for how we can build and scale AI workloads — from conversational agents, complex task automation, to custom LLM-backed experiences that 'just work'. LangGraph Platform is a service for deploying and scaling LangGraph applications, with an opinionated API for building agent UXs, plus an integrated developer studio.\", \"score\": 0.19191799}]\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  human_assistance (call_FxTR8gH4mhyxfHlOw9hCJyQn)\n",
      " Call ID: call_FxTR8gH4mhyxfHlOw9hCJyQn\n",
      "  Args:\n",
      "    VaultFindings: LangGraph version 0.1 was announced along with LangGraph Cloud, indicating its initial release.\n",
      "    Web_results: LangGraph first version 0.1 was announced in a blog post, suggesting it is the earliest version released.\n"
     ]
    }
   ],
   "source": [
    "user_input = (\n",
    "    \"Can you look up when LangGraph was released? \"\n",
    "    \"When you have the answer, use the human_assistance tool for review.\"\n",
    ")\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "events = graph.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": user_input}]},\n",
    "    config,\n",
    "    stream_mode=\"values\",\n",
    ")\n",
    "for event in events:\n",
    "    if \"messages\" in event:\n",
    "        event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  human_assistance (call_Uu8FKy9U09bHUTLTXerAFdf5)\n",
      " Call ID: call_Uu8FKy9U09bHUTLTXerAFdf5\n",
      "  Args:\n",
      "    name: LangGraph\n",
      "    birthday: 2025-02-06\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: human_assistance\n",
      "\n",
      "Made a correction: {'name': 'LangGraph', 'birthday': 'Jan 17, 2024'}\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The release date of LangGraph is January 17, 2024.\n"
     ]
    }
   ],
   "source": [
    "human_command = Command(\n",
    "    resume={\n",
    "        \"name\": \"LangGraph\",\n",
    "        \"birthday\": \"Jan 17, 2024\",\n",
    "    },\n",
    ")\n",
    "\n",
    "events = graph.stream(human_command, config, stream_mode=\"values\")\n",
    "for event in events:\n",
    "    if \"messages\" in event:\n",
    "        event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this right after your human_assistance tool, where you indicated\n",
    "\n",
    "@tool\n",
    "def extract_vault_data(\n",
    "    tool_call_id: Annotated[str, InjectedToolCallId]\n",
    ") -> str:\n",
    "    \"\"\"Extract data from vault findings and prepare for updates.\"\"\"\n",
    "    # This tool doesn't need parameters since it reads from state\n",
    "    \n",
    "    # Return a Command to update the state\n",
    "    return Command(update={\n",
    "        \"messages\": [ToolMessage(\"Vault data processed successfully\", tool_call_id=tool_call_id)]\n",
    "    })\n",
    "\n",
    "# Define a node function that processes vault documents\n",
    "def vault_processor(state: State):\n",
    "    \"\"\"Node to process retrieved vault documents and extract relevant information.\"\"\"\n",
    "    # Extract documents from state\n",
    "    vault_findings = state.get(\"VaultFindings\", [])\n",
    "    query_intent = state.get(\"Query_results\", {})\n",
    "    \n",
    "    # Process documents - here we're keeping it simple as you requested\n",
    "    processed_docs = []\n",
    "    \n",
    "    for doc in vault_findings:\n",
    "        # Extract key information\n",
    "        uuid = doc.metadata.get(\"uuid\", \"\")\n",
    "        title = doc.metadata.get(\"title\", \"Untitled\")\n",
    "        content = doc.page_content\n",
    "        \n",
    "        # Store in a simple dict\n",
    "        processed_docs.append({\n",
    "            \"uuid\": uuid,\n",
    "            \"title\": title,\n",
    "            \"content\": content,\n",
    "            \"metadata\": doc.metadata\n",
    "        })\n",
    "    \n",
    "    # Return updated state\n",
    "    return {\n",
    "        \"processed_vault_docs\": processed_docs,\n",
    "        \"primary_doc\": processed_docs[0] if processed_docs else None\n",
    "    }\n",
    "\n",
    "# Now, modify your graph to include this node\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "\n",
    "webtool = TavilySearchResults(max_results=7)\n",
    "tools = [webtool, human_assistance, extract_vault_data]  # Add the new tool\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "\n",
    "def chatbot(state: State):\n",
    "    message = llm_with_tools.invoke(state[\"messages\"])\n",
    "    assert len(message.tool_calls) <= 1\n",
    "    return {\"messages\": [message]}\n",
    "\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "graph_builder.add_node(\"vault_processor\", vault_processor)  # Add vault processor node\n",
    "\n",
    "tool_node = ToolNode(tools=tools)\n",
    "graph_builder.add_node(\"tools\", tool_node)\n",
    "\n",
    "# Define a condition to route to vault processor when we have vault findings\n",
    "def should_process_vault(state):\n",
    "    \"\"\"Route to vault processor if we have VaultFindings.\"\"\"\n",
    "    if state.get(\"VaultFindings\"):\n",
    "        return \"vault_processor\"\n",
    "    return \"chatbot\"\n",
    "\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"chatbot\",\n",
    "    tools_condition,\n",
    ")\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"tools\",\n",
    "    should_process_vault,\n",
    ")\n",
    "graph_builder.add_edge(\"vault_processor\", \"chatbot\")\n",
    "graph_builder.add_edge(START, \"chatbot\")\n",
    "\n",
    "memory = MemorySaver()\n",
    "graph = graph_builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
